# -*- coding: utf-8 -*-
"""Q_Learning_nyctaxi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19yFfn7LRpy7whiZhmvfRS6gtz2fFKKqC
"""

!pip install gymnasium
!pip install gymnasium[toy_text]

import gymnasium as gym
import numpy as np

"""1. CREATE THE ENVIRONMENT"""

# Taxi-v3 is a classic discrete environment where:
# - The taxi must pick up a passenger at one location
# - Drop them off at another location
# - Avoid illegal moves and minimize steps

env = gym.make("Taxi-v3")

# Number of possible states and actions

n_states = env.observation_space.n  # e.g. 500
n_actions = env.action_space.n # e.g. 6

"""2. INITIALIZE Q-TABLE"""

# Q-table shape: [num_states x num_actions]
# Q[s, a] will store the estimated value of taking action a in state s

Q = np.zeros((n_states, n_actions))

"""3. HYPERPARAMETERS"""

alpha = 0.1 # Learning rate: how much new info overrides old info
gamma = 0.99 # Discount factor: importance of future rewards
epsilon = 1.0 # Initial exploration rate (start very exploratory)
epsilon_min = 0.05 # Minimum exploration (we always keep some randomness)
epsilon_decay = 0.999 # How fast epsilon decays per episode
episodes = 5000 # Number of training episodes

# For tracking progress (optional)
reward_history = []

"""4. TRAINING LOOP"""

for ep in range(episodes):
  # env.reset() in Gymnasium returns (observation, info)
  state, info = env.reset()
  done = False
  total_reward = 0

  while not done:
    # Epsilon-greedy policy
    # With probability epsilon: choose randon action (explore)
    # Otherwise: choose best known action (exploit)
    if np.random.random() < epsilon:
      action = np.argmax(Q[state]) # Best action from Q-table for this state

    # Take the action in the environment
    # Gymnasium step() returns:
    # next_step, reward, terminated, truncated, info
    # done = terminated (natural end) OR truncated (time limit / interrupt)

    next_state, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated

    # Q-learning updated rule:
    # Q(s,a) <- Q(s,a) + α [ r + γ max_a' Q(s', a') − Q(s,a) ]

    best_next_action = np.max(Q[next_state]) # max over a' Q(s', a')
    td_target = reward + gamma * best_next_action
    td_error = td_target - Q[state, action]
    Q[state, action] = Q[state, action] + alpha * td_error

    # Move to next state
    state = next_state
    total_reward += reward

  # Decay epsilon after each episode (gradually reduce exploration)

  epsilon = max(epsilon_min, epsilon * epsilon_decay)

  reward_history.append(total_reward)

  # Print occasional progress
  if (ep + 1) % 500 == 0:
    avg_last_500 = np.mean(reward_history[-500:])
    print(f"Episode {ep+1}/{episodes} - Avg reward (last 500): {avg_last_500:.2f}, epsilon: {epsilon:.3f}")

print("Trainig complete!")

"""**Test the trained agent**"""

# Re-create environment with text (ANSI) rendering
# This makes env.render() return a string visualization
env = gym.make("Taxi-v3", render_mode="ansi")

state, info = env.reset()
done = False
total_reward = 0
step_count = 0

print("Running one trained episode:\n")

while not done:
  # Choose the best action according to learned Q-table (pure exploitation)
  action = np.argmax(Q[state])

  # Take action in environment
  next_state, reward, terminated, truncated, info = env.step(action)
  done = terminated or truncated
  total_reward += reward
  step_count += 1

  # Render the environment as text and print it
  frame_str = env.render() # returns a string in ansi mode
  print(frame_str)
  print("-" * 40)

  state = next_state

print("Episode finished in", step_count, "steps with total reward:", total_reward)

"""**Quick Sanity Check**"""

import matplotlib.pyplot as plt

# Simple moving average over window of 100 episodes
window = 100

if len(reward_history) >= window:
  moving_avg = np.convolve(reward_history, np.ones(window)/window, mode='valid')
  plt.plot(moving_avg)
  plt.xlabel("Episode")
  plt.ylabel(f"Average reward (window={window})")
  plt.title("Training progression - Taxi-v3 Q-learning")
  plt.show()
else:
  print("Not enough episodes to plot moving average.")

